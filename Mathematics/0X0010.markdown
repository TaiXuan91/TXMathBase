# 王元，数学大辞典，控制论与信息论

《数学大辞典》的第十六部分介绍控制论与信息论，这一部分涉及的学科是相当新的，大部分是20世纪才建立的。

控制论以微分方程为基础，为工业控制系统建模并提供控制策略。因为有众多应用场景，这一部分算是和统计学一样的“显学”。对于这一部分我的了解不多，只是大致的知道时域、频域、传递函数这些概念。

信息论则着重对“信息”进行描述和建模。在信息论以前，“信息”也是一个模糊的概念。1948年香农的划时代论文《通信的数学理论》，将这个概念用数学方式阐明，并开创了信息论。信息论可以算是个出道就巅峰的理论，因为学习信息论，学的大部分内容其实都是香农那篇论文的内容。其最主要的思想就是将“信息”这个概念关联到概率上。用随机变量概率分布的函数——熵——来度量随机变量的不确定程度。信息则被定义为不确定程度的减少量。这里我们简单举个例子：

假设这样一个实验，随机抛一枚均匀的硬币A，然后由主持人查看抛硬币结果，把另一枚硬币B摆成和A相反的样子，然后给你看B的样子。直觉上这里我们虽然没有看到A但是能准确得知A的情况，因为观测B给了我们关于A的“信息”。在信息论中，A抛出正反两种情况的概率都为1/2，按照信息熵公式计算得熵为1。B出现正反两种情况的概率也都为1/2，因而熵也是1。A，B的联合分布中因为二者同时正面、同时反面两种情况不存在，算下来的联合熵也是1。按照互信息公式计算将A，B各自的熵加和之后减去联合分布的熵，就是1+1-1得1。这个结果恰好等于A的熵。而互信息表示知道两个事件中一者的结果时，另一个者的不确定程度减少量。这样，当知道B的结果后，A的不确定程度归0，对于观测者来说由于B提供的信息，A成了非常确定的事件。

个人建议要理解信息熵就找香农的论文原文看一看，《数学大辞典》中的论述也非常清楚。网上和现实中经常有人在“确定性定义”和“不确定性定义”上来回折腾，说什么香农的定义和一般理解的信息不同，把别人和自己都绕晕了。实际上香农吧“不确定性定义”是针对“熵”的，而不是针对“信息”的。很多人没有搞清楚这俩的区别就绕进去了。熵越大，不确定性就越大，但这和我们直观上的信息没有关系，香农这里也没有用“信息”一词。直到定义“互信息”的时候，才用了“信息”一词。而互信息是熵的差值，或者说减少量（两个事件各自熵的总和减去联合分布的熵）。所以互信息是，不确定性的减少量。换句话说，信息是用来减少不确定性的东西。而像“越混乱信息越大”，“信息是不确定度的度量”这些说法严格来说都是错的。（这里我没有说不确定性的减少就是确定性的增加，因为“非不确定性”不一定就是“确定性”，香农只定义了不确定性，而没有定义确定性。）

书中该部分的第三节是密码学，这个领域在如今的计算机和通讯领域应用非常广泛。我个人对这个领域也是很有兴趣的，但可惜没有系统学过所以就不做介绍了。